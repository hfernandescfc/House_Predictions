{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "House_Price_Prediction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN8dhqahud1lxUJG5+SiyGr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hfernandescfc/House_Predictions/blob/main/House_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "61BtJwKElaNB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from scipy.stats import norm, skew #for some statistics\n",
        "pd.set_option('display.max_columns', None)\n",
        "import missingno as msno\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/'\n",
        "\n",
        "test = 'test.csv'\n",
        "train = 'train.csv'"
      ],
      "metadata": {
        "id": "ClDjoaCSp26g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(path + train)\n",
        "test_df = pd.read_csv(path + test)\n",
        "\n",
        "copy_train = train_df.copy()\n",
        "copy_test = test_df.copy()\n",
        "\n",
        "dfs_copied = [copy_train, copy_test]"
      ],
      "metadata": {
        "id": "eprMAC_mpBlK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "9396e3f6-4de6-4d61-bba6-b256ede5ccb5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6e32bc6a97c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcopy_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcopy_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FRAMEWORK\n",
        "\n",
        "1. Understand Data\n",
        "2. Data Preprocessing\n",
        "3. EDA\n",
        "4. Feature Engineering*\n",
        "5. Model Deployment\n",
        "6. Evaluate\n",
        "7. Redeploy"
      ],
      "metadata": {
        "id": "QbtIJgC7wpCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanging Data"
      ],
      "metadata": {
        "id": "_MaJBZl28QuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train data frame shape:\" + str(copy_train.shape))\n",
        "print(\"\\n\"+\"-\" * 100)\n",
        "print(\"Test data frame shape:\" + str(copy_test.shape))\n",
        "print(\"\\n\"+\"-\" * 100)"
      ],
      "metadata": {
        "id": "4BuL8wHTq2Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(copy_train.columns)\n",
        "print('\\n')\n",
        "print(copy_train.info())"
      ],
      "metadata": {
        "id": "UZv6LOZOvyQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_train.head()"
      ],
      "metadata": {
        "id": "PjK9hBG5wa7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Cleaning"
      ],
      "metadata": {
        "id": "0yOGh8fJGgqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtypes = ['category', 'int', 'float64']\n",
        "\n",
        "for df in dfs_copied:\n",
        "  df = df.drop(columns = 'Id', inplace=True)\n",
        "\n",
        "\n",
        "Categorical = copy_train.select_dtypes(include='object')\n",
        "\n",
        "Numerical = copy_train.columns[~copy_train.columns.isin([Categorical])]\n",
        "\n",
        "for d in dfs_copied:\n",
        "  for c in Categorical.columns:\n",
        "    d[c] = d[c].astype('category')"
      ],
      "metadata": {
        "id": "qboiB6gsKmu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with Missing data"
      ],
      "metadata": {
        "id": "UzsbAIy-8K9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_totals = copy_train.isnull().sum().sort_values(ascending=False)\n",
        "\n",
        "missing_percentuals = (copy_train.isnull().sum()/len(copy_train)).sort_values(ascending=False)\n",
        "\n",
        "missing_df = pd.concat([missing_totals, missing_percentuals], axis = 1, keys = ['Values Missing', 'Percent Missing'])\n",
        "\n",
        "missing_columns = missing_df[missing_df['Values Missing'] > 0].index\n",
        "\n",
        "display(missing_df[missing_df['Values Missing'] > 0])\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(missing_columns)"
      ],
      "metadata": {
        "id": "qodKm7iG7t1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although it is a good practice to keep as much data as possible, I decided to remove the features which had more than 50% of missing values."
      ],
      "metadata": {
        "id": "L7nUM1G90yhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Threeshold = 0.5\n",
        "\n",
        "Beyond_Threeshold = missing_df[missing_df['Percent Missing'] > Threeshold].index\n",
        "\n",
        "copy_train = copy_train.drop(columns = Beyond_Threeshold)\n",
        "\n",
        "print(copy_train.columns)\n",
        "\n"
      ],
      "metadata": {
        "id": "M-tzidbsxxtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtypes = ['category', 'int', 'float']\n",
        "\n",
        "Categorical = copy_train.select_dtypes(include='object')\n",
        "\n",
        "Numerical = copy_train.columns[~copy_train.columns.isin([Categorical])]\n",
        "\n",
        "for d in dfs_copied:\n",
        "  for c in Categorical.columns:\n",
        "    d[c] = d[c].astype('category')"
      ],
      "metadata": {
        "id": "jsZdIZ625Pqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I'll proceed to analyse each feature individually in order to preserve as much information as I can"
      ],
      "metadata": {
        "id": "7to9xMnE8-uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def order_mapping(df, columns, dict):\n",
        "\n",
        "  df[columns] = df[columns].cat.add_categories(\"NA\").fillna(value=\"NA\")\n",
        "\n",
        "  df[columns] = df[columns].map(dict)\n",
        "\n",
        "  df[columns] = df[columns].fillna(0)\n"
      ],
      "metadata": {
        "id": "7T-3uzZCK0Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FireplaceQu"
      ],
      "metadata": {
        "id": "AY-aJmq7Dmt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#copy_train['FireplaceQu'].value_counts(ascending=False)\n",
        "\n",
        "grade_dict = {'NA': 0, 'Po':1, 'Fa':2, 'TA' : 3, 'Gd':4, 'EX' :5}\n",
        "\n",
        "#g_without_nan = g.cat.add_categories(\"D\").fillna(\"D\")\n",
        "\n",
        "order_mapping(copy_train, 'FireplaceQu', grade_dict)\n",
        "\n",
        "#copy_train['FireplaceQu'] = copy_train['FireplaceQu'].fillna(0)\n",
        "\n",
        "fig = px.box(copy_train, x = 'FireplaceQu', y = 'SalePrice', points = 'all')\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "r9Iy7roKyfEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LotFrontage"
      ],
      "metadata": {
        "id": "aCFFuySKD8vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#copy_train.groupby(by='Neighborhood')['LotFrontage'].median()\n",
        "\n",
        "fig, axs = plt.subplots(ncols = 2)\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "figure(figsize=(8, 4), dpi=100)\n",
        "\n",
        "g = sns.boxplot(data = copy_train, x = 'Neighborhood', y = 'LotFrontage', ax = axs[0] )\n",
        "\n",
        "g.set_xticklabels(labels = copy_train['Neighborhood'], rotation=90)\n",
        "\n",
        "g= sns.countplot(data= copy_train, x = 'Neighborhood', ax = axs[1])\n",
        "\n",
        "g.set_xticklabels(labels = copy_train['Neighborhood'], rotation=90)\n",
        "\n",
        "#plt.xticks(rotation = 70)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "copy_train[\"LotFrontage\"] = copy_train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
        "    lambda x: x.fillna(x.median()))\n",
        "\n",
        "#copy_train = copy_train.drop(columns='LotFrontage')"
      ],
      "metadata": {
        "id": "XbhZV69QD-Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Garage Features"
      ],
      "metadata": {
        "id": "YcFt4-NBJ-PJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "Garage_dict = {'NA': 0, 'Po':1, 'Fa':2, 'TA' : 3, 'Gd':4, 'EX' :5}\n",
        "\n",
        "order_mapping(copy_train, 'GarageCond', Garage_dict)\n",
        "\n",
        "order_mapping(copy_train, 'GarageQual', Garage_dict)\n",
        "\n",
        "copy_train['GarageYrBlt'] = copy_train['GarageYrBlt'].fillna(0)\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "\n",
        "garages = ['GarageType', 'GarageFinish']\n",
        "\n",
        "for col in garages:\n",
        "  copy_train[col] = copy_train[col].cat.add_categories(\"NA\").fillna(value=\"NA\")\n",
        "  \n",
        "\"\"\"transformer = make_column_transformer((OneHotEncoder(), \n",
        "                                       garages),\n",
        "                                       remainder = 'passthrough')\n",
        "\n",
        "transformed = transformer.fit_transform(copy_train)\n",
        "\n",
        "transformed_copy_train = pd.DataFrame(transformed, columns = transformer.get_feature_names())\"\"\"\n",
        "\n",
        "#for col in garages:\n",
        "#  copy_train[col] = enc.fit_transform(copy_train[[col]])"
      ],
      "metadata": {
        "id": "cAUjFF47KAjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basement Features"
      ],
      "metadata": {
        "id": "_YO7DQB98kU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#'BsmtFinType2', 'BsmtExposure', 'BsmtQual', 'BsmtCond', 'BsmtFinType1'\n",
        "\n",
        "bsmt_cols = ['BsmtFinType2', 'BsmtExposure', 'BsmtQual', 'BsmtCond', 'BsmtFinType1']\n",
        "\n",
        "ord_bsmt = ['BsmtQual', 'BsmtCond']\n",
        "\n",
        "bsmt_cat = list(set(bsmt_cols)-set(ord_bsmt))\n",
        "\n",
        "for col in ord_bsmt :\n",
        "  order_mapping(copy_train, col, grade_dict)\n",
        "\n",
        "for col in bsmt_cat:\n",
        "  copy_train[col] = copy_train[col].cat.add_categories(\"NA\").fillna(value=\"NA\")\n",
        "\n",
        "\"\"\"transformer = make_column_transformer((OneHotEncoder(), \n",
        "                                       bsmt_cat),\n",
        "                                       remainder = 'passthrough')\n",
        "\n",
        "transformed = transformer.fit_transform(copy_train)\n",
        "\n",
        "transformed_copy_train = pd.DataFrame(transformed, columns = transformer.get_feature_names())\"\"\"\n",
        "\n",
        "#copy_train[]\n",
        "\n",
        "#for col in list(set(bsmt_cols)-set(ord_bsmt)):\n",
        "#  copy_train[col] =labelencoder.fit_transform(copy_train[col])\n",
        "\n",
        "#for col in bsmt_cols:\n",
        "#  print(str(col) + '\\n') \n",
        "#  print(transformed_copy_train[col].value_counts(ascending=False))"
      ],
      "metadata": {
        "id": "UgIB9EG58nqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transformed_copy_train.head()"
      ],
      "metadata": {
        "id": "qyTnLI_XcQXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masonry"
      ],
      "metadata": {
        "id": "2iq7mvuobeel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "copy_train['MasVnrType'] = copy_train['MasVnrType'].cat.add_categories(\"NA\").fillna('NA')\n",
        "\n",
        "copy_train['MasVnrArea'] = copy_train.groupby('Neighborhood')['MasVnrArea'].transform(\n",
        "    lambda x: x.fillna(x.median()))\n",
        "\n"
      ],
      "metadata": {
        "id": "PpcERserbl9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Electrical"
      ],
      "metadata": {
        "id": "V1uf1Nv4pkGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "copy_train['Electrical'] = copy_train['Electrical'].fillna(copy_train['Electrical'].mode()[0])"
      ],
      "metadata": {
        "id": "3gf9zYWEsE2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_train.isna().sum().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "KQCoV_lyUyjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No more missing values :)"
      ],
      "metadata": {
        "id": "TYJtxkSSU_hQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ],
      "metadata": {
        "id": "Px0tfzY9qRa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section is where we can get a better grasp of the data. The goal here is to check the relationship between the features so we can properly build the right model for the kind of data that is available."
      ],
      "metadata": {
        "id": "md8EH-uvMhTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = copy_train.drop(columns='SalePrice')\n",
        "\n",
        "Y_train = copy_train.loc[:, 'SalePrice']\n",
        "\n",
        "Categorical = X_train.select_dtypes(include='category')\n",
        "\n",
        "Numerical = X_train.select_dtypes(include=['int', 'float'])\n",
        "\n"
      ],
      "metadata": {
        "id": "vsRwtQ8QswMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Numerical.describe()"
      ],
      "metadata": {
        "id": "wadXgF-xo5-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(Numerical.columns)):\n",
        "  sns.scatterplot(x=Numerical.iloc[:,i], y =Y_train)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "bqEFJbPg4FKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to add another collumn of plots showing the countplot of each categorical feature"
      ],
      "metadata": {
        "id": "LfEPxXijJx5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "copy_train = copy_train.drop(copy_train[copy_train['GrLivArea'] > 4500].index)"
      ],
      "metadata": {
        "id": "Q6wDVECjj_Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(Categorical.columns)):\n",
        "  sns.boxplot(x=Categorical.iloc[:,i], y =Y_train)\n",
        "  plt.title(Categorical.columns[i])\n",
        "  plt.xticks(rotation = 90)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "llB62CZAbgdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the scatter and box plots, it seems that there may be some significant outliers present on our data. We will come back to it if our residual plots indicates that this is affecting our models."
      ],
      "metadata": {
        "id": "Twd7PV-Z64FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr = X_train.corr()\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
        "\n",
        "mask = np.zeros_like(X_train.corr(), dtype=bool) \n",
        "\n",
        "mask[np.triu_indices_from(mask)] = True \n",
        "\n",
        "plt.title('Pearson Correlation Matrix',fontsize=25)\n",
        "\n",
        "f = sns.heatmap(X_train.corr(), square = True, cmap=\"BuGn\", linewidths = 0.2, mask = mask)#linewidths=0.25,vmax=0.7,square=True,cmap=\"BuGn\", #\"BuGn_r\" to reverse \n",
        "            #linecolor='w',annot=True,annot_kws={\"size\":8},mask=mask,cbar_kws={\"shrink\": .9})\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T1DsRzkLD4tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(Y_train, fit = norm)\n",
        "\n",
        "mu, sigma = norm.fit(Y_train)\n",
        "\n",
        "plt.title('Sale Price Distribution',fontsize=20)\n",
        "\n",
        "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
        "            loc='best')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GH9N-yHmOkIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Histogram of SalePrice shows that the distribution is skewed to the right. Although normality is not an assumption for the linear models that will be deployed in this notebook, it might be benefical to transform the dependent variable in order to achieve better predictions."
      ],
      "metadata": {
        "id": "_izXpBjHGljZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_corr = copy_train.corr()\n",
        "\n",
        "y_corr = y_corr['SalePrice'].sort_values(ascending=False)\n",
        "\n",
        "top_10_y = y_corr.nlargest(11)[1:]\n",
        "\n",
        "print(top_10_y)"
      ],
      "metadata": {
        "id": "hIBAFG08FrN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From a simple analysis of the features it is clear that OverallQual and GrLivArea are the predictors with the highest linear correlation to SalePrice. "
      ],
      "metadata": {
        "id": "10XUsVttHLAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering"
      ],
      "metadata": {
        "id": "TJrHa1lhemtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will not try to do anything fancy here although I believe I could achieve some better results by combining some of the predictors, the focus here will be on adjusting data to fit our linear models."
      ],
      "metadata": {
        "id": "4pHA_zTF7YS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(copy_train.info())"
      ],
      "metadata": {
        "id": "TMMn9lhQX1b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_train['BsmtCond'] = copy_train['BsmtCond'].astype('int64') #fix"
      ],
      "metadata": {
        "id": "Jdpdwl2LZ1vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transform remaining ordinal features to numerical\n",
        "\n",
        "remaining_ordinal = ['ExterQual', 'ExterCond', 'HeatingQC', 'KitchenQual']\n",
        "\n",
        "\n",
        "\n",
        "for col in remaining_ordinal:\n",
        "  order_mapping(copy_train, col, grade_dict)\n"
      ],
      "metadata": {
        "id": "oAki6DjB8HFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OneHotEncoding remaning categorical variables\n",
        "\n",
        "Categorical = copy_train.select_dtypes(include='category').columns\n",
        "\n",
        "transformer = make_column_transformer((OneHotEncoder(), \n",
        "                                       Categorical),\n",
        "                                       remainder = 'passthrough')\n",
        "\n",
        "transformed = transformer.fit_transform(copy_train)\n",
        "\n",
        "transformed_copy_train = pd.DataFrame(transformed, columns = transformer.get_feature_names())"
      ],
      "metadata": {
        "id": "553srOJWajN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_copy_train.info()"
      ],
      "metadata": {
        "id": "QC8dgIF4biU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = transformed_copy_train.drop(columns='SalePrice')\n",
        "\n",
        "Y_train = transformed_copy_train.loc[:, 'SalePrice']\n",
        "\n",
        "#Important for some linear models to have the variance of each feature in the same order of magnitude\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scale = StandardScaler()\n",
        "\n",
        "scale.fit(X_train[Numerical.columns])\n",
        "\n",
        "X_train[Numerical.columns] = scale.transform(X_train[Numerical.columns])"
      ],
      "metadata": {
        "id": "Uukew8b0CCam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Deployment"
      ],
      "metadata": {
        "id": "cKHbApBAJBaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def rmse_cv(model):\n",
        "    rmse= np.sqrt(-cross_val_score(model, X_train, Y_train, scoring=\"neg_mean_squared_error\", cv = 5))\n",
        "    return(rmse)"
      ],
      "metadata": {
        "id": "nsCILu-VGpZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ridge = Ridge()\n",
        "\n",
        "\n",
        "alphas = [0.1, 1, 5, 10, 50]\n",
        "\n",
        "cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n",
        "            for alpha in alphas]"
      ],
      "metadata": {
        "id": "nUYVVKoNQRuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_ridge = pd.Series(cv_ridge, index = alphas)\n",
        "cv_ridge.plot(title = \"Ridge RMSE per alpha\")\n",
        "plt.xlabel(\"alpha\")\n",
        "plt.ylabel(\"rmse\")"
      ],
      "metadata": {
        "id": "8VaG2rsKQKPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ridge = Ridge(alpha = 10)\n",
        "\n",
        "model_ridge.fit(X_train, Y_train)\n"
      ],
      "metadata": {
        "id": "8gquXxNIDPgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model_ridge.predict(X_train)\n",
        "\n",
        "residuals = Y_train - preds\n",
        "\n",
        "res_frame = pd.DataFrame({'preds':preds, 'residuals': residuals})\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (6.0, 6.0)\n",
        "\n",
        "sns.residplot(x = preds, y = residuals)\n",
        "\n",
        "plt.title('Residuals plot')\n",
        "\n",
        "plt.ylabel('Residuals')\n",
        "\n",
        "plt.xlabel('Predictions')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z4jbe_wQDfrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall the residuals plot indicates that the error terms have non constant variance, wich means, heteroscedasticity. To tackle this problem the common approach is transforming the response variable Y. It also suggests that it's possible to increase the model performance by removing outliers."
      ],
      "metadata": {
        "id": "kLeof2zfJwhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "copy_train['SalePrice'] = np.log(copy_train['SalePrice'])"
      ],
      "metadata": {
        "id": "bDcEkh2WnAIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r-3fReCewlVQ"
      }
    }
  ]
}